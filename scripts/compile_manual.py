#!/usr/bin/env python3
import csv
from mylog import mylog
from pathlib import Path
import re
from lxml.html import fromstring as lxsoup

SRC_DIR = Path('data', 'collected', 'sic_manual', 'description',)
TARGET_PATH = Path('data', 'compiled', 'sic_manual.csv')

COMPILED_HEADERS = (
    'sic_code', 'sic_name',
    'group_code', 'group_name',
    'division_code', 'division_name',
    'url',
    'sic_description',
    'sic_examples',
)

def parse_description_page(txt):
    _rx_division = r'Division ([A-Z]): (.+)'
    def _extract_division(soup):
        dtext = next(t for t in soup.xpath('//a[contains(@title, "Division")]/@title') if re.match(_rx_division, t))
        return re.match(_rx_division, dtext).groups()

    _rx_group = r'Major Group (\d+): (.+)'
    def _extract_group(soup):
        dtext = next(t for t in soup.xpath('//a[contains(@title, "Major Group")]/@title') if re.match(_rx_group, t))
        return re.match(_rx_group, dtext).groups()

    _rx_sic= r'Description for (\d{4}): (.+)'
    def _extract_sic(soup):
        # try:
        # except StopIteration:
        #     import pdb; pdb.set_trace()
        dtext = next(t for t in soup.xpath('//h2[contains(text(), "Description")]/text()') if re.match(_rx_sic, t))
        return re.match(_rx_sic, dtext).groups()

    def _extract_sic_desc(soup):
        els = soup.xpath('//span[contains(@class, "blueTen")]/text()')
        if len(els) == 0:
            return ""
        elif len(els) > 1:
            import pdb; pdb.set_trace()
        else:
            return els[0]

    def _extract_sic_examples(soup):
        examples = soup.xpath('//span[contains(@class, "blueTen")]/following-sibling::ul[1]/li/text()')
        if len(examples) == 0:
            return ""
        else:
            return '\n'.join(e.strip() for e in examples)


    soup = lxsoup(txt)
    d = {}
    d['division_code'], d['division_name'] = _extract_division(soup)
    d['group_code'], d['group_name'] = _extract_group(soup)
    d['sic_code'], d['sic_name'] = _extract_sic(soup)
    d['sic_description'] = _extract_sic_desc(soup)
    d['sic_examples'] = _extract_sic_examples(soup)
    return d

def make_desc_page_url(page_path):
    idstr = page_path.stem
    return f"https://www.osha.gov/pls/imis/sic_manual.display?id={idstr}&tab=description"


def main():
    def _get_desc_paths():
        return sorted(SRC_DIR.glob('*.html'), key=lambda p: int(p.stem))

    def _read_text(path):
        return path.read_bytes().decode('latin-1')

    srcpaths = _get_desc_paths()
    mylog(f"{len(srcpaths)} files in {SRC_DIR}", label="Gathered")

    TARGET_PATH.parent.mkdir(exist_ok=True, parents=True)
    with open(TARGET_PATH, 'w') as target_file:
        mylog(TARGET_PATH, label="Writing to")
        target = csv.DictWriter(target_file, fieldnames=COMPILED_HEADERS)
        target.writeheader()


        for i, srcpath in enumerate(srcpaths):
            txt = _read_text(srcpath)
            record = parse_description_page(txt)
            record['url'] = make_desc_page_url(srcpath)

            target.writerow(record)
            if i % 50 == 0:
                mylog(f"{i} out of {len(srcpaths)} â€“ {srcpath}" ,label="Parsing")


if __name__ == '__main__':
    main()
